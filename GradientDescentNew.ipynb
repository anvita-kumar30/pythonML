{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqe9vhgAfNqb",
        "outputId": "d8542809-7ec3-4311-dba6-8a294a806ba4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final x1: 0.3485932765528664, Final x2: 0.9940779541059878\n",
            "Final Predictions: [1.16837459 2.33674918 3.0080848  4.17645939]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "class GradientDescentMSE:\n",
        "    def __init__(self, lr=0.01, n_iters=1000):\n",
        "        self.lr = lr\n",
        "        self.n_iters = n_iters\n",
        "        self.x1 = None\n",
        "        self.x2 = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Initialize parameters\n",
        "        self.x1 = np.random.randn()  # Initialize x1\n",
        "        self.x2 = np.random.randn()  # Initialize x2\n",
        "\n",
        "        for _ in range(self.n_iters):\n",
        "            # Compute predictions\n",
        "            y_pred = self.x1 * X[:, 0] + self.x2 * X[:, 1]\n",
        "\n",
        "            # Compute gradients for MSE\n",
        "            grad_x1 = (2/len(y)) * np.sum((y_pred - y) * X[:, 0])\n",
        "            grad_x2 = (2/len(y)) * np.sum((y_pred - y) * X[:, 1])\n",
        "\n",
        "            # Update parameters\n",
        "            self.x1 = self.x1 - self.lr * grad_x1\n",
        "            self.x2 = self.x2 - self.lr * grad_x2\n",
        "\n",
        "        return self.x1, self.x2\n",
        "\n",
        "    def objective_function(self, X):\n",
        "        return self.x1 * X[:, 0] + self.x2 * X[:, 1]\n",
        "\n",
        "\n",
        "# Example dataset with two features\n",
        "X = np.array([[0.5, 1.0], [1.0, 2.0], [1.5, 2.5], [2.0, 3.5]])  # Features\n",
        "y = np.array([1.5, 2.5, 3.0, 4.0])  # True values\n",
        "\n",
        "# Initialize and run gradient descent\n",
        "gd_mse = GradientDescentMSE(lr=0.01, n_iters=1000)\n",
        "final_x1, final_x2 = gd_mse.fit(X, y)\n",
        "final_predictions = gd_mse.objective_function(X)\n",
        "\n",
        "print(f\"Final x1: {final_x1}, Final x2: {final_x2}\")\n",
        "print(f\"Final Predictions: {final_predictions}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class GradientDescentCustom:\n",
        "    def __init__(self, lr=0.01, n_iters=1000):\n",
        "        self.lr = lr        # Learning rate\n",
        "        self.n_iters = n_iters  # Number of iterations\n",
        "        self.x1 = None      # To store parameter x1\n",
        "        self.x2 = None      # To store parameter x2\n",
        "\n",
        "    def objective_function(self, x1, x2):\n",
        "        # Objective function: f(x1, x2) = -x1^3 + 6*x2^2\n",
        "        return -x1**3 + 6*x2**2\n",
        "\n",
        "    def compute_gradients(self, x1, x2):\n",
        "        # Partial derivative of f with respect to x1: df/dx1 = -3 * x1^2\n",
        "        grad_x1 = -3 * x1**2\n",
        "        # Partial derivative of f with respect to x2: df/dx2 = 12 * x2\n",
        "        grad_x2 = 12 * x2\n",
        "        return grad_x1, grad_x2\n",
        "\n",
        "    def fit(self):\n",
        "        # Random initialization of x1 and x2\n",
        "        self.x1 = np.random.randn()\n",
        "        self.x2 = np.random.randn()\n",
        "\n",
        "        # Perform gradient descent\n",
        "        for i in range(self.n_iters):\n",
        "            # Compute gradients\n",
        "            grad_x1, grad_x2 = self.compute_gradients(self.x1, self.x2)\n",
        "\n",
        "            # Update parameters\n",
        "            self.x1 = self.x1 - self.lr * grad_x1\n",
        "            self.x2 = self.x2 - self.lr * grad_x2\n",
        "\n",
        "            # Compute the value of the objective function\n",
        "            objective_value = self.objective_function(self.x1, self.x2)\n",
        "\n",
        "            # Print the values at each step\n",
        "            print(f\"Iteration {i+1}: x1 = {self.x1:.6f}, x2 = {self.x2:.6f}, Objective = {objective_value:.6f}\")\n",
        "\n",
        "        return self.x1, self.x2\n",
        "\n",
        "# Initialize and run gradient descent\n",
        "gd = GradientDescentCustom(lr=0.01, n_iters=5)\n",
        "final_x1, final_x2 = gd.fit()\n",
        "\n",
        "print(f\"\\nFinal Values: x1 = {final_x1}, x2 = {final_x2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNWFyo4t3h6T",
        "outputId": "cb5f0f92-6c0d-42b2-970a-b041242546d6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: x1 = -0.898021, x2 = 0.800601, Objective = 4.569970\n",
            "Iteration 2: x1 = -0.873827, x2 = 0.704529, Objective = 3.645396\n",
            "Iteration 3: x1 = -0.850920, x2 = 0.619985, Objective = 2.922412\n",
            "Iteration 4: x1 = -0.829198, x2 = 0.545587, Objective = 2.356123\n",
            "Iteration 5: x1 = -0.808571, x2 = 0.480117, Objective = 1.911705\n",
            "\n",
            "Final Values: x1 = -0.8085710531841143, x2 = 0.48011657594137136\n"
          ]
        }
      ]
    }
  ]
}